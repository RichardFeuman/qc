package com.example

import LogAnalysis.LogRecord.unapply

import org.apache.spark.rdd.RDD
import org.apache.spark.sql
import org.apache.spark.sql.functions.{col, when}
import org.apache.spark.sql.{DataFrame, SparkSession}

import scala.util.Try

object LogAnalysis2 extends App {

  /*
  1. считать логи из файла в rdd. Для этого изучите содержимое файла, структуру данных и выведите свои правила,
  по которым будет происходить парсинг данных.

  Вполне возможно,  что не все данные получится считать на основании описанных вами правил,
  поэтому следует отобразить статистику по успешно считанным строчкам и неуспешно - для этого достаточно
  посчитать количество строк для каждого случая.



  2. следующее задание - найти количество записей для каждого кода ответа (response)



  3. следует собрать статистику по размеру ответа (bytes): сколько всего байт было отправлено
  (сумма всех значений), среднее значение, максимальное и минимальное значение.



  4. подсчитать количество уникальных хостов (host), для которых представлена статистика



  5. найти топ-3 наиболее часто встречающихся хостов (host). Результат анализа, выводимый на экран,
  должен включать в себя хоста и подсчитанную частоту встречаемости этого хоста.



  6. определите дни недели (понедельник, вторник и тд), когда чаще всего выдавался ответ (response) 404.
  В статистике отобразите день недели (в каком виде отображать день остается на ваше усмотрение, например,
  Mon, Tue ) и сколько раз был получен ответ 404. Достаточно отобразить топ-3 дней.

   */

  val spark = SparkSession.builder()
    .appName("log analysis app")
    .master("local")
    .getOrCreate()


  // ,host,time,method,url,response,bytes


  def readCsvToRDD(path: String, hasHeader: Boolean = true, sep: String =",") = {

    require(!path.isEmpty && new java.io.File(path).exists(), s"path '$path' does not exists")
    require(sep.nonEmpty, "sep not passed")

    val rdd = spark.sparkContext.textFile(path)

    if (hasHeader) {
      val header: String = rdd.first()
      rdd.filter(_ != header).map(line => tryWrapToClass(line.split(sep).slice(1, line.split(sep).length)))
    } else {
      rdd.map(line => tryWrapToClass(line.split(sep).slice(1, line.split(sep).length))) // индекс не нужен
    }

  }

  def distplayStatAfterRead(successAndNotRecordsCnt: SucceccAndNotRecordsCnt) = {

    val successAndNotRecordsCntCp = successAndNotRecordsCnt.copy()

    println(s"num of successfully readed records : ${successAndNotRecordsCntCp.successRecordsCnt}")

    println(s"num of not readed records : ${successAndNotRecordsCntCp.notSucessRecordsCnt}")
  }

  case class LogRecord(host: String ,time: String, method: String,url: String, response: Option[Int],bytes : String )

  case class SucceccAndNotRecordsCnt(successRecordsCnt: Int, notSucessRecordsCnt: Int)

  object LogRecord {
    def unapply(lr: LogRecord): Option[Int] = Some(lr.response.getOrElse(-1))

  }

  def tryWrapToClass(rddLine: Array[String]) = {
      LogRecord(host = rddLine(0) ,
        time = rddLine(1),
        method = rddLine(2),
        url = rddLine(3),
        response= Try(rddLine(4).toInt).toOption,
        bytes = rddLine(5))

  }




  val logsRDD = readCsvToRDD("resources/data/logs_data.csv") // t1
  logsRDD.collect().filter(x=>x.response.isEmpty).foreach(println)



  // 2. следующее задание - найти количество записей для каждого кода ответа (response)

  val aggFunc : (Int, Int) => Int = (acc, count) => acc + count // t2

  def aggregateLogData(rdd: RDD[Int], reduceOp: (Int, Int) => Int) = {

    require(rdd != null, "Input RDD cannot be null")
    require(rdd.count()>0, "Input RDD cannot be empty")
    require(reduceOp != null, "Reduce operation function cannot be null")

      val rddCopy = rdd
      val pairedRdd = rddCopy.map{line=>(line, 1)}

      val aggregatedRdd = pairedRdd.reduceByKey(reduceOp)
      aggregatedRdd

  }


  val extractFieldFromRddBeforeGetResponseCountStat: (RDD[LogRecord]) => RDD[Int] = (rddWithLogData) => {

    rddWithLogData.map{lr=> LogRecord.unapply(lr).getOrElse(-1) }
  }

  val extractFieldFromRddBeforeCalculateBytesStat: (RDD[LogRecord]) => RDD[Int] = (rddWithLogData) => {

    rddWithLogData.map{lr=> lr.bytes.toInt }

  }


  val rddWithResponseCodeOnly = extractFieldFromRddBeforeGetResponseCountStat(logsRDD)
  val codeResponseCntRdd = aggregateLogData(rddWithResponseCodeOnly, aggFunc)

  codeResponseCntRdd.collect().foreach(println)

  /*
  3. следует собрать статистику по размеру ответа (bytes):
    сколько всего байт было отправлено (сумма всех значений), среднее значение,
    максимальное и минимальное значение.
   */



  def getAverageOfBytesVolume(rdd: RDD[Int]) = {
    require(rdd.count() > 0, "passed rdd is empty")
    require(rdd.isInstanceOf[RDD[Int]], "not rdd was passed")
    val pair = rdd.map{b=> (b, 1)}.reduce((u, v) => (u._1 + v._1, u._2 + v._2 ))
    pair._1/pair._2

  }

  def getSumOfBytesVolume(rdd: RDD[Int]) = {
    require(rdd.count() > 0, "passed rdd is empty")
    require(rdd.isInstanceOf[RDD[Int]], "not rdd was passed")
    rdd.sum()


  }

  def getMinOfBytesVolume(rdd: RDD[Int]) = {
    require(rdd.count() > 0, "passed rdd is empty")
    require(rdd.isInstanceOf[RDD[Int]], "not rdd was passed")
    rdd.min()

  }

  def getMaxOfBytesVolume(rdd: RDD[Int]) = {
    require(rdd.count() > 0, "passed rdd is empty")
    require(rdd.isInstanceOf[RDD[Int]], "not rdd was passed")
    rdd.max()

  }

  def getStatOfBytesResponse(bytesRDD: RDD[Int],
                             fToGetAvg: (RDD[Int]) => Int,
                             fToGetMin: (RDD[Int]) => Int,
                             fToGetMax: (RDD[Int]) => Int,
                             fToGetSum: (RDD[Int]) => Double
                            ) ={
    require(bytesRDD.count() > 0, "passed rdd is empty")
    require(bytesRDD.isInstanceOf[RDD[Int]], "not rdd was passed")
    (fToGetAvg(bytesRDD) , fToGetMin(bytesRDD), fToGetMax(bytesRDD), fToGetSum(bytesRDD))
  }

  val rddWithBytesOnly = extractFieldFromRddBeforeCalculateBytesStat(logsRDD)
  println(getStatOfBytesResponse(rddWithBytesOnly,getAverageOfBytesVolume, getMinOfBytesVolume, getMaxOfBytesVolume, getSumOfBytesVolume))



}

